
# 使用

通过简单的操作步骤，就可以让用户拥有：
- 数据仓库（数据采集、清洗）
- 商业智能
- 推荐引擎

### 步骤

- 设定输入的数据格式
  - 设定数据原格式，以及抽取规则，建立入库表
  - 启动数据接收服务（要想想怎样才能更简单收集数据）
- 设置要使用的商业模型
  - 选择模型
  - 填写模型所需的数据源（关联到已经创建的数据表）
  - 设置运行模式（定时？）
- 实际使用
  - API调用
  - 界面操作

# 接口

对外提供两种接口，batch & stream

很多的预测算法，都需要预先训练模型，此时使用batch模式
而对于需要上下文进行判断的数据应用，stream会是更好的选择

### 流式接口/实时任务模型

流式接口的计算用于对需要实时计算（如行为预测）场景进行计算，
接口通过消息队列接收信息，计算出结果后，推送到消息队列中发布
出去，客户端可以通过http/websocket等多种方式，在前端服务器
获取结果。

实时模型，往往需要验证后再上线，考虑实现回测框架，以及风控系统，保证实时模型的正常运行

### 批量请求接口/离线任务模型

批量请求接口用于客户端收集了足够的信息后，进行模型训练，模型训练后，
一次性往服务器推送上下文的所需信息，预测下一步的结果。

用于管理模型的离线计算，需要数据的输入，考虑到数据会持续更新，而模型也
需要根据业务的发展持续演进，需要考虑实现模型的定时更新方案，为了保证模型的持续演进，
还需要考虑模型的持续更新和AB切换

考虑到数据输入有冷启动的情况（数据量不够导致无法完成启动），此时暂时无法提供服务，
需要具备计算过程上下文保存的能力，方便第三方查询目前是否有可用模型

- 创建：创建离线任务
- 启动：把离线任务启动
- 查询：查询离线任务的状态
- 暂停：把上下文保存起来

批量请求接口需要模型经过计算，并且有测试结果后，才可以正常工作，
请求接口应该包含该部分逻辑的描述。

考虑以下方式的接口：
- 图片识别
  - 图片识别数据提交接口
    - 数据
    - 分类结果
  - 图片识别结果接口
    - 提交：数据
    - 返回：分类real
- 识别年龄
- 对象检测
- 名片格式化

# 数据

对数据源的抽象定义，提供字段和接口（？考虑使用spark.DataFrame）

### 如何建立模型

系统需要用户输入特征，输入希望预测的结果，因需要用户参与，
需考虑提供用户咨询服务，避免用户陷入困惑

- 用户选择希望预测的结果
- 用户挑选输入的数据
- 启动模型训练，获得模型的评估量度
- 模型上线，启动模型服务

### 消息业务定义

/where/who/when/what/how/why
/厂家/业务/{服|区域}/用户/行为{购买|点击|...}/参数/上下文

如何处理第三方数据更新？
/厂家/业务/商品/行为{添加|更新}/参数

### 数据例子

| user | action1     | action2       | action3     |
|------|-------------|---------------|-------------|
|user1 | login:time  | buy:item1     | award:item7 |
|user2 | login:time  | buy:item3     |   .....     |

| item   | cost  | type  |
|--------|-------|-------|
| item1  | cost1 | type3 |

### 消息保存

适应用户弹性格式存储需求？

- 如何提取参数内容：用户自定义参数
- 如何提取上下文：用户自定义参数

### 数据常用格式

配合Restful的流行，json被广泛采用，json的特点是弹性格式，用户随时可能
根据业务的发展需要，改写对象的存储格式，从而导致数据不可使用

为了同时满足数据的弹性，以及数据交换的标准性，数据需要考虑metadata的设计，
如版本号，标准字段，如版本号，可以获得的服务

### 数据二次处理

采用结构化方式进行数据保存，有助于把数据分布式存储在数据库中，且快速提取
所需的数据内容，但因为数据归集时，可能会面临数据格式未知的情况，此时无法
把数据有效抽取保存

作为中心服务，无法应对业务端积极变化，带来的变化，这种支持肯定是滞后的，
应对这样的变化，除了中心服务端需要有前瞻性以外，还需要中心服务端可以容纳
业务端的变化，不至于“封杀”业务端的可能性

应对变化，采取二级处理策略，首先，把用户的原始数据落地保存，然后，再把数据
投递到ETL系统中，进行数据的清洗（校验/过滤/转换），需要特别注意的是，
在二级处理的阶段，需要有监控报告，关注数据处理能力

### 数据仓库

不使用二次处理结果的数据原因，是为了实现数据的交换，此时数据仓库会是更好的选择

考虑使用metadata实现数据的汇集，数据版本控制/需要字段...

能够通过数据二次处理后，进入到数据仓库的数据，才可以被使用，数据仓库面临着
如何提供实时的处理结果给客户的需求，包括查询的实时反馈，和关注信息的实时推送

我们可以把数据分为两类，一种是状态数据，一种是行为数据（操作数据），进入到
数据仓库中的数据，此时的建模和计算，都是基于数据仓库，数据仓库还有一个特点，
来自于远端的数据，进入数据仓库后，不再可变

如何解决状态数据在数据仓库的保存？状态数据可以采用插入的方式，查询时，只读取
最新的状态信息


### 结果预测

- /厂家/业务/{服|区域}/用户/行为模型/参数 => 结果
- 用户真实行为结果记录
- 后台离线服务持续计算效果？？跟踪模型的效率？
